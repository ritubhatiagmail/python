{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seshat Duplicate Algo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5dXgGJOi9X5Y",
        "wEdFkp7B-PfG"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritubhatiagmail/python/blob/master/Seshat_Duplicate_Algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dXgGJOi9X5Y",
        "colab_type": "text"
      },
      "source": [
        "# Imoprting libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh6Ic721Xrdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d938ee75-7f75-4973-e13d-687535760aea"
      },
      "source": [
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/23/833e0620753a36cb2f18e2e4a4f72fd8c49c123c3f07744b69f8a592e083/sentence-transformers-0.3.0.tar.gz (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.2MB/s \n",
            "\u001b[?25hCollecting transformers>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.5.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.2->sentence-transformers) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.2->sentence-transformers) (3.0.4)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.0-cp36-none-any.whl size=86756 sha256=0bd70601202f43ee90dde72f86ac919bc179d2c5d858e2d4fabe1d07c31dcd0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/23/85/85d6a9a6c68f0625a1ecdaad903bb0a78df058c10cf74f9de4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=fe72b0c13c5e113403b4143434a47f52d8abe709d5d9de8ff8eb4fc0be56ff91\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.0 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:45<00:00, 8.98MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/f9/0626bbdb322e3a078d968e87e3b01341e7890544de891d0cb613641220e6/ipython-autotime-0.1.tar.bz2\n",
            "Building wheels for collected packages: ipython-autotime\n",
            "  Building wheel for ipython-autotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-cp36-none-any.whl size=1832 sha256=2b420dd7b6a0bdc24c202f50d1081c9170a0c57db9edd2fe06b33c54b092bddb\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/df/81/2db1e54bc91002cec40334629bc39cfa86dff540b304ebcd6e\n",
            "Successfully built ipython-autotime\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 2s (2,715 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 144465 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/d8/521db389ff0aae32035bfda6ed39cb2c2e28521c47015f6431f07460c50a/pytesseract-0.3.4.tar.gz\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from pytesseract) (7.0.0)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.4-py2.py3-none-any.whl size=13431 sha256=81f425352d97c5cb6347332af2280c44f7a678453cf68d799d7ffc22120ab86c\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/a0/7596d2e0a73cf0aeffd6f6170862c4e73f3763b7827e48691a\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.4\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "518JPWCeXBH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f820d39-1085-4da3-af86-f26d4ffe6717"
      },
      "source": [
        "import spacy\n",
        "import string\n",
        "import re\n",
        "import pytesseract\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image\n",
        "from scipy import signal\n",
        "import cv2\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import pandas as pd\n",
        "from scipy import spatial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 635 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEdFkp7B-PfG",
        "colab_type": "text"
      },
      "source": [
        "# Getting all files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED2nny0927HL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9995b8a2-c6f6-40ec-aee5-a68aaa4dd33d"
      },
      "source": [
        "files_P, files_P_op, files_P_sol = [], [], []\n",
        "files_B, files_B_op, files_B_sol = [] , [], []\n",
        "files_M, files_M_op, files_M_sol = [] , [], []\n",
        "files_C, files_C_op, files_C_sol = [] ,[], []\n",
        "for subdir, dirs, files in os.walk(r'/content/drive/My Drive/Duplicate_Algo_280620'):\n",
        "    files.sort()\n",
        "    for filename in files:\n",
        "       if filename.endswith(\".GIF\"):\n",
        "        if ((filename[8] == '0') & (filename[9] == '0')):\n",
        "          if filename.startswith(\"P\"):\n",
        "            files_P.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"B\"):\n",
        "            files_B.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"M\"):\n",
        "            files_M.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"C\"):\n",
        "            files_C.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "        if ((filename[8] == '0') & (filename[9] == '1')):\n",
        "          if filename.startswith(\"P\"):\n",
        "            files_P_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"B\"):\n",
        "            files_B_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"M\"):\n",
        "            files_M_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"C\"):\n",
        "            files_C_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "        if ((filename[8] == '0') & (filename[9] == '2')):\n",
        "          if filename.startswith(\"P\"):\n",
        "            files_P_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"B\"):\n",
        "            files_B_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"M\"):\n",
        "            files_M_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"C\"):\n",
        "            files_C_op.append(os.path.abspath(os.path.join(subdir, filename)))   \n",
        "        if ((filename[8] == '0') & (filename[9] == '3')):\n",
        "          if filename.startswith(\"P\"):\n",
        "            files_P_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"B\"):\n",
        "            files_B_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"M\"):\n",
        "            files_M_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"C\"):\n",
        "            files_C_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "        if ((filename[8] == '0') & (filename[9] == '4')):\n",
        "          if filename.startswith(\"P\"):\n",
        "            files_P_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"B\"):\n",
        "            files_B_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"M\"):\n",
        "            files_M_op.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"C\"):\n",
        "            files_C_op.append(os.path.abspath(os.path.join(subdir, filename)))                                         \n",
        "        if ((filename[8] == '9') & (filename[9] == '9')):\n",
        "          if filename.startswith(\"P\"):\n",
        "            files_P_sol.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"B\"):\n",
        "            files_B_sol.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"M\"):\n",
        "            files_M_sol.append(os.path.abspath(os.path.join(subdir, filename)))\n",
        "          elif filename.startswith(\"C\"):\n",
        "            files_C_sol.append(os.path.abspath(os.path.join(subdir, filename)))              "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 10.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWtuMaw-WuLB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "3cf5331c-661c-4a63-9157-cd54266bbdef"
      },
      "source": [
        "print(len(files_P), len(files_B), len(files_M), len(files_C) )\n",
        "print(len(files_P_op), len(files_B_op), len(files_M_op), len(files_C_op) )\n",
        "print(len(files_P_sol), len(files_B_sol), len(files_M_sol), len(files_C_sol))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 20 19 20\n",
            "76 80 76 76\n",
            "19 0 20 20\n",
            "time: 3.01 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ks_8Sh0Wa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "57766320-bcbd-45df-9e4e-421c1c4891f4"
      },
      "source": [
        "files_P[0][-12:-7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'08354'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "stream",
          "text": [
            "time: 14.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdbynWicDDZ1",
        "colab_type": "text"
      },
      "source": [
        "# Compare image Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMOeWJ7bDBmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "9cf95f33-2101-4d82-d2d7-ec41d5e5f374"
      },
      "source": [
        "similar_df_Pi = pd.DataFrame(columns=['Id_Q1', 'Id_Q2', 'Similarity_Ques'])\n",
        "import cv2\n",
        "for i in range(len(files_P)):\n",
        "  for j in range(i+1, len(files_P)):\n",
        "    gif = imageio.get_reader(files_P[i])\n",
        "    gif2 = imageio.get_reader(files_P[j])\n",
        "    for frame in gif:\n",
        "      image_1 = frame[:,:,0]\n",
        "    for frame in gif2:\n",
        "      image_2 = frame[:,:,0]\n",
        "    first_image_hist = cv2.calcHist([image_1], [0], None, [256], [0, 256])\n",
        "    second_image_hist = cv2.calcHist([image_2], [0], None, [256], [0, 256])\n",
        "\n",
        "    img_hist_diff = cv2.compareHist(first_image_hist, second_image_hist, cv2.HISTCMP_BHATTACHARYYA)\n",
        "    img_template_probability_match = cv2.matchTemplate(first_image_hist, second_image_hist, cv2.TM_CCOEFF_NORMED)[0][0]\n",
        "    img_template_diff = 1 - img_template_probability_match\n",
        "\n",
        "    # taking only 10% of histogram diff, since it's less accurate than template method\n",
        "    commutative_image_diff = (img_hist_diff / 10) + img_template_diff\n",
        "    similar_df_Pi = similar_df_Pi.append({'Id_Q1':files_P[i][-14:-7], 'Id_Q2':files_P[j][-14:-7], 'Similarity_Ques':commutative_image_diff*1000}, ignore_index=True)\n",
        "    #print( commutative_image_diff*10000)\n",
        "\n",
        "similar_df_Pi = similar_df_Pi.sort_values(by=['Similarity_Ques'])\n",
        "similar_df_Pi.reset_index(drop=True, inplace=True)\n",
        "similar_df_Pi = similar_df_Pi.round(2)\n",
        "similar_df_Pi2 = similar_df_Pi.iloc[:20,:]\n",
        "similar_df_Pi2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id_Q1</th>\n",
              "      <th>Id_Q2</th>\n",
              "      <th>Similarity_Ques</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P-03149</td>\n",
              "      <td>P-08402</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P-08354</td>\n",
              "      <td>P-08394</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P-50273</td>\n",
              "      <td>P-50193</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P-08394</td>\n",
              "      <td>P-08402</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P-50273</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>P-08394</td>\n",
              "      <td>P-03149</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-04563</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>P-08354</td>\n",
              "      <td>P-08402</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-04899</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>P-04406</td>\n",
              "      <td>P-22405</td>\n",
              "      <td>0.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>P-08354</td>\n",
              "      <td>P-03149</td>\n",
              "      <td>1.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>P-50273</td>\n",
              "      <td>P-22405</td>\n",
              "      <td>1.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>P-22405</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>1.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>P-50273</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>1.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>P-01118</td>\n",
              "      <td>P-02504</td>\n",
              "      <td>1.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>P-04406</td>\n",
              "      <td>P-02504</td>\n",
              "      <td>1.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-07970</td>\n",
              "      <td>1.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>P-03149</td>\n",
              "      <td>P-04563</td>\n",
              "      <td>1.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>1.42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Id_Q1    Id_Q2  Similarity_Ques\n",
              "0   P-03149  P-08402             0.30\n",
              "1   P-08354  P-08394             0.54\n",
              "2   P-50273  P-50193             0.80\n",
              "3   P-08394  P-08402             0.83\n",
              "4   P-50273  P-50040             0.84\n",
              "5   P-50193  P-50293             0.84\n",
              "6   P-08394  P-03149             0.92\n",
              "7   P-50193  P-04563             0.95\n",
              "8   P-08354  P-08402             0.95\n",
              "9   P-00202  P-04899             0.96\n",
              "10  P-04406  P-22405             0.99\n",
              "11  P-08354  P-03149             1.07\n",
              "12  P-50273  P-22405             1.08\n",
              "13  P-22405  P-50293             1.18\n",
              "14  P-50273  P-50293             1.19\n",
              "15  P-01118  P-02504             1.26\n",
              "16  P-04406  P-02504             1.33\n",
              "17  P-00202  P-07970             1.39\n",
              "18  P-03149  P-04563             1.40\n",
              "19  P-50193  P-50040             1.42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "stream",
          "text": [
            "time: 10.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SYl_WHh-WQe",
        "colab_type": "text"
      },
      "source": [
        "Change the image path to the desired path to save result images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ATP9l38H94u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec3093d2-2d3d-4e08-b8a8-57b27aab62ef"
      },
      "source": [
        "image_path = '/content/drive/My Drive/Duplicate_Algo_280620/Results3(image)'\n",
        "count=1\n",
        "for j in range(len(similar_df_Pi2)):\n",
        "  for i in range(len(files_P)):\n",
        "    if files_P[i][-14:-7] == similar_df_Pi2.iloc[j,0]:\n",
        "      A = plt.imread(files_P[i])\n",
        "      #print(files_P[i][-14:-7])\n",
        "    if files_P[i][-14:-7] == similar_df_Pi2.iloc[j,1]:\n",
        "      #print(files_P[i][-14:-7])\n",
        "      B = plt.imread(files_P[i])\n",
        "  #print(\"Other\")\n",
        "  width = A.shape[1]\n",
        "  height = A.shape[0]\n",
        "  dsize = (width, height)\n",
        "  output = cv2.resize(B, dsize)\n",
        "  C = np.hstack((A, output))\n",
        "  #cv2_imshow(C)\n",
        "  cv2.imwrite(os.path.join(image_path,('P' + str(count) + '_' +  (str(similar_df_Pi2.iloc[j,2]))[:5]+\".jpg\")),C)\n",
        "  count+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 153 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXSGcp2ADGb5",
        "colab_type": "text"
      },
      "source": [
        "# Ocr detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MISYdL8bwUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14baf63b-08d6-4e0a-c988-e0889e26ffe5"
      },
      "source": [
        "def simi_fn2(X,Y):\n",
        "  dataSetI = X\n",
        "  dataSetII = Y\n",
        "  result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
        "  return (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.53 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtylgkiB297B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "0c4a368d-de3a-4b4f-eaef-e6d1c23cee2e"
      },
      "source": [
        "df = pd.DataFrame(columns=['Id', 'Questions', 'Options', 'Solution'])\n",
        "for filename in files_P:\n",
        "  temp=[]\n",
        "  for i in range(len(files_P_op)):\n",
        "    if files_P_op[i][-12:-7] == filename[-12:-7]:\n",
        "      gif = imageio.get_reader(files_P_op[i])\n",
        "      for frame in gif:\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "          info = pytesseract.image_to_string(frame)\n",
        "          temp.append(info)\n",
        "  temp.sort()\n",
        "  \n",
        "  temp2 = ' '.join(temp)\n",
        "  #temp2 = temp2.replace(\" \",\"\")\n",
        "  temp3=[]\n",
        "  for i in range(len(files_P_sol)):\n",
        "    if files_P_sol[i][-12:-7] == filename[-12:-7]:\n",
        "      gif = imageio.get_reader(files_P_sol[i])\n",
        "      for frame in gif:\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "          info = pytesseract.image_to_string(frame)\n",
        "          temp3.append(info)\n",
        "  temp3.sort()\n",
        "  temp4 = ' '.join(temp3)\n",
        "  #temp4 = temp4.replace(\" \",\"\")\n",
        "  gif = imageio.get_reader(filename)\n",
        "  for frame in gif:\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    info = pytesseract.image_to_string(frame)\n",
        "    #info = info.replace(\" \",\"\")\n",
        "  df = df.append({'Id':filename[-14:-7],'Questions':info, 'Options':temp2, 'Solution':temp4},ignore_index=True)\n",
        "df['Questions'].astype(str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     A bullet of mass m and speed v hits a pendulum...\n",
              "1     A bullet of mass m and speed v hits a pendulum...\n",
              "2     If the condenser of capacitance 2F shown in th...\n",
              "3     In the condenser show in the circuit is charge...\n",
              "4     A plane electromagnetic wave of frequency 25MH...\n",
              "5     A plane electromagnetic wave of frequency 50 M...\n",
              "6          X-rays can not penetrate through a sheet of:\n",
              "7                              X-rays do not penetrate:\n",
              "8     Two particles X and Y having equal charges, af...\n",
              "9     Two particles A and B having equal charges aft...\n",
              "10    A small hole is made at the bottom of a symmet...\n",
              "11    A small hole is made at the bottom of a symmet...\n",
              "12    If potential difference of 10,000 volt is appl...\n",
              "13    A potential difference of 20 kV is applied acr...\n",
              "14    A inductor 20 mH a capacitor 100 uF and a resi...\n",
              "15    A inductor 20 mH a capacitor 100 uF and a resi...\n",
              "16    An inductor 20 mH, a capacitor 50 pF and a res...\n",
              "17    When the hydrogen atom emits a photon in going...\n",
              "18    Two cars A and B start racing at the same time...\n",
              "19    Two cars A and B start racing at the same time...\n",
              "Name: Questions, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "stream",
          "text": [
            "time: 1min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TBL3hDsUohi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33306985-253c-4d52-a6a7-bb46f1a7f8db"
      },
      "source": [
        "df[\"Questions\"] = df[\"Questions\"].str.lower()\n",
        "df[\"Options\"] = df[\"Options\"].str.lower()\n",
        "df[\"Solution\"] = df[\"Solution\"].str.lower()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.54 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb03COgQbC8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7e7ae148-955a-4d74-ac4d-6f650edb3243"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "df[\"Questions\"] = df[\"Questions\"].apply(lambda text: remove_stopwords(text))\n",
        "df[\"Options\"] = df[\"Options\"].apply(lambda text: remove_stopwords(text))\n",
        "df[\"Solution\"] = df[\"Solution\"].apply(lambda text: remove_stopwords(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "time: 15.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEtLoeKqX_xi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8badff26-dddf-47af-9bf0-e89750617e9a"
      },
      "source": [
        "questions = df['Questions'].to_list()\n",
        "options = df['Options'].to_list()\n",
        "solutions = df['Solution'].to_list()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.42 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNnfQH6tX1zU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6d12569-1ba1-48cb-99af-60de7a30e0af"
      },
      "source": [
        "questions_embeddings = model.encode(questions)\n",
        "options_embeddings = model.encode(options)\n",
        "solutions_embeddings = model.encode(solutions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 11.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0sqZ2f5lGAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "d2a4c359-0cd2-4c9c-b871-7c20aecea76f"
      },
      "source": [
        "similar_df_P = pd.DataFrame(columns=['Id_Q1', 'Id_Q2', 'Similarity_Ques', 'Similarity_Opt', 'Similarity_Sol'])\n",
        "for i in range(df.shape[0]):\n",
        "  for j in range(i+1, df.shape[0]):\n",
        "    score1 = simi_fn2(questions_embeddings[i], questions_embeddings[j])\n",
        "    score2 = simi_fn2(options_embeddings[i], options_embeddings[j])\n",
        "    score3 = simi_fn2(solutions_embeddings[i], solutions_embeddings[j])\n",
        "    if score1 > 0.7:\n",
        "         similar_df_P = similar_df_P.append({'Id_Q1':df.iloc[i,0], 'Id_Q2':df.iloc[j,0], 'Similarity_Ques':score1, 'Similarity_Opt':score2, 'Similarity_Sol':score3}, ignore_index=True)\n",
        "print(similar_df_P)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Id_Q1    Id_Q2  Similarity_Ques  Similarity_Opt  Similarity_Sol\n",
            "0   P-08354  P-08394         0.998420        0.946974        0.820198\n",
            "1   P-00202  P-04899         0.906202        0.633348        0.473455\n",
            "2   P-00202  P-50193         0.720889        0.503206        0.856458\n",
            "3   P-00202  P-50293         0.730348        0.545546        0.900034\n",
            "4   P-00202  P-50040         0.739488        0.482057        0.614653\n",
            "5   P-04406  P-50273         0.890714        0.828299        0.367926\n",
            "6   P-04406  P-01118         0.714858        0.456922        0.234293\n",
            "7   P-02491  P-02546         0.954623        0.448601        0.994799\n",
            "8   P-01118  P-22405         0.965802        0.731623        0.579725\n",
            "9   P-03149  P-08402         0.986351        1.000000        0.951037\n",
            "10  P-02551  P-02504         0.766654        0.678473        0.662023\n",
            "11  P-02551  P-50193         0.755492        0.645219        0.681305\n",
            "12  P-02551  P-50293         0.752156        0.729623        0.760150\n",
            "13  P-02551  P-50040         0.703485        0.698626        0.454667\n",
            "14  P-02504  P-50193         0.768474        0.625267        0.754868\n",
            "15  P-02504  P-50293         0.764277        0.636612        0.752085\n",
            "16  P-02504  P-50040         0.706163        0.738292        0.541048\n",
            "17  P-50193  P-50293         0.992577        0.874748        0.954379\n",
            "18  P-50193  P-50040         0.952246        0.791380        0.727407\n",
            "19  P-50293  P-50040         0.943016        0.869099        0.675074\n",
            "20  P-50293  P-04563         0.702447        0.527201        0.756557\n",
            "21  P-07970  P-21468         0.992839        0.111049        0.147846\n",
            "time: 124 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FUhDyYRxf0J",
        "colab_type": "text"
      },
      "source": [
        "1.   21 dup, 144 ms, 42 for ocr(with 4 channel) \n",
        "2.   21 dup, 151 ms, 41.8 for ocr(With4) without spaces\n",
        "3.   21 dup, 147, 42.5 1 channel\n",
        "2.   21 dup, 127, 38.3, 1 channel with space\n",
        "1.   21 dup, 151, 38.3+4, lowercase\n",
        "1.   16 dup, removed punctuations\n",
        "2.   22 dup, 150, 38.2+4+13, lowercase, removed stopwords\n",
        "1.   18 dup, 167ms, removed 10 most common words\n",
        "2.   22 dup,167 ms, removed 2 most common words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOMqb-NI_fH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1d85bab-f1e9-4739-d58d-3ced661e2172"
      },
      "source": [
        "image_path = '/content/drive/My Drive/Duplicate_Algo_280620/Results2'\n",
        "count=1\n",
        "for j in range(len(similar_df_P)):\n",
        "  for i in range(len(files_P)):\n",
        "    if files_P[i][-14:-7] == similar_df_P.iloc[j,0]:\n",
        "      A = plt.imread(files_P[i])\n",
        "    if files_P[i][-14:-7] == similar_df_P.iloc[j,1]:\n",
        "      B = plt.imread(files_P[i])\n",
        "  width = A.shape[1]\n",
        "  height = A.shape[0]\n",
        "  dsize = (width, height)\n",
        "  output = cv2.resize(B, dsize)\n",
        "  C = np.hstack((A, output))\n",
        "  #cv2_imshow(C)\n",
        "  cv2.imwrite(os.path.join(image_path,('P' + str(count) + '_' +  (str(similar_df_P.iloc[j,2]))[:5]+\".jpg\")),C)\n",
        "  count+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 137 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OFyeXx5xqxC",
        "colab_type": "text"
      },
      "source": [
        "# Ocr 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IUSvk3apahb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "760bd5e6-d9e4-4183-af31-93a7c733dd62"
      },
      "source": [
        "similar_df_P['combine']= similar_df_P.Id_Q1 + ',' + similar_df_P.Id_Q2\n",
        "similar_df_P"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id_Q1</th>\n",
              "      <th>Id_Q2</th>\n",
              "      <th>Similarity_Ques</th>\n",
              "      <th>Similarity_Opt</th>\n",
              "      <th>Similarity_Sol</th>\n",
              "      <th>combine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P-08354</td>\n",
              "      <td>P-08394</td>\n",
              "      <td>0.998420</td>\n",
              "      <td>0.946974</td>\n",
              "      <td>0.820198</td>\n",
              "      <td>P-08354,P-08394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-04899</td>\n",
              "      <td>0.906202</td>\n",
              "      <td>0.633348</td>\n",
              "      <td>0.473455</td>\n",
              "      <td>P-00202,P-04899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-50193</td>\n",
              "      <td>0.720889</td>\n",
              "      <td>0.503206</td>\n",
              "      <td>0.856458</td>\n",
              "      <td>P-00202,P-50193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.730348</td>\n",
              "      <td>0.545546</td>\n",
              "      <td>0.900034</td>\n",
              "      <td>P-00202,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.739488</td>\n",
              "      <td>0.482057</td>\n",
              "      <td>0.614653</td>\n",
              "      <td>P-00202,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>P-04406</td>\n",
              "      <td>P-50273</td>\n",
              "      <td>0.890714</td>\n",
              "      <td>0.828299</td>\n",
              "      <td>0.367926</td>\n",
              "      <td>P-04406,P-50273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>P-04406</td>\n",
              "      <td>P-01118</td>\n",
              "      <td>0.714858</td>\n",
              "      <td>0.456922</td>\n",
              "      <td>0.234293</td>\n",
              "      <td>P-04406,P-01118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>P-02491</td>\n",
              "      <td>P-02546</td>\n",
              "      <td>0.954623</td>\n",
              "      <td>0.448601</td>\n",
              "      <td>0.994799</td>\n",
              "      <td>P-02491,P-02546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>P-01118</td>\n",
              "      <td>P-22405</td>\n",
              "      <td>0.965802</td>\n",
              "      <td>0.731623</td>\n",
              "      <td>0.579725</td>\n",
              "      <td>P-01118,P-22405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>P-03149</td>\n",
              "      <td>P-08402</td>\n",
              "      <td>0.986351</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.951037</td>\n",
              "      <td>P-03149,P-08402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>P-02551</td>\n",
              "      <td>P-02504</td>\n",
              "      <td>0.766654</td>\n",
              "      <td>0.678473</td>\n",
              "      <td>0.662023</td>\n",
              "      <td>P-02551,P-02504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>P-02551</td>\n",
              "      <td>P-50193</td>\n",
              "      <td>0.755492</td>\n",
              "      <td>0.645219</td>\n",
              "      <td>0.681305</td>\n",
              "      <td>P-02551,P-50193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>P-02551</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.752156</td>\n",
              "      <td>0.729623</td>\n",
              "      <td>0.760150</td>\n",
              "      <td>P-02551,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>P-02551</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.703485</td>\n",
              "      <td>0.698626</td>\n",
              "      <td>0.454667</td>\n",
              "      <td>P-02551,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>P-02504</td>\n",
              "      <td>P-50193</td>\n",
              "      <td>0.768474</td>\n",
              "      <td>0.625267</td>\n",
              "      <td>0.754868</td>\n",
              "      <td>P-02504,P-50193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>P-02504</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.764277</td>\n",
              "      <td>0.636612</td>\n",
              "      <td>0.752085</td>\n",
              "      <td>P-02504,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>P-02504</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.706163</td>\n",
              "      <td>0.738292</td>\n",
              "      <td>0.541048</td>\n",
              "      <td>P-02504,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.992577</td>\n",
              "      <td>0.874748</td>\n",
              "      <td>0.954379</td>\n",
              "      <td>P-50193,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.952246</td>\n",
              "      <td>0.791380</td>\n",
              "      <td>0.727407</td>\n",
              "      <td>P-50193,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>P-50293</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.943016</td>\n",
              "      <td>0.869099</td>\n",
              "      <td>0.675074</td>\n",
              "      <td>P-50293,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>P-50293</td>\n",
              "      <td>P-04563</td>\n",
              "      <td>0.702447</td>\n",
              "      <td>0.527201</td>\n",
              "      <td>0.756557</td>\n",
              "      <td>P-50293,P-04563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>P-07970</td>\n",
              "      <td>P-21468</td>\n",
              "      <td>0.992839</td>\n",
              "      <td>0.111049</td>\n",
              "      <td>0.147846</td>\n",
              "      <td>P-07970,P-21468</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Id_Q1    Id_Q2  ...  Similarity_Sol          combine\n",
              "0   P-08354  P-08394  ...        0.820198  P-08354,P-08394\n",
              "1   P-00202  P-04899  ...        0.473455  P-00202,P-04899\n",
              "2   P-00202  P-50193  ...        0.856458  P-00202,P-50193\n",
              "3   P-00202  P-50293  ...        0.900034  P-00202,P-50293\n",
              "4   P-00202  P-50040  ...        0.614653  P-00202,P-50040\n",
              "5   P-04406  P-50273  ...        0.367926  P-04406,P-50273\n",
              "6   P-04406  P-01118  ...        0.234293  P-04406,P-01118\n",
              "7   P-02491  P-02546  ...        0.994799  P-02491,P-02546\n",
              "8   P-01118  P-22405  ...        0.579725  P-01118,P-22405\n",
              "9   P-03149  P-08402  ...        0.951037  P-03149,P-08402\n",
              "10  P-02551  P-02504  ...        0.662023  P-02551,P-02504\n",
              "11  P-02551  P-50193  ...        0.681305  P-02551,P-50193\n",
              "12  P-02551  P-50293  ...        0.760150  P-02551,P-50293\n",
              "13  P-02551  P-50040  ...        0.454667  P-02551,P-50040\n",
              "14  P-02504  P-50193  ...        0.754868  P-02504,P-50193\n",
              "15  P-02504  P-50293  ...        0.752085  P-02504,P-50293\n",
              "16  P-02504  P-50040  ...        0.541048  P-02504,P-50040\n",
              "17  P-50193  P-50293  ...        0.954379  P-50193,P-50293\n",
              "18  P-50193  P-50040  ...        0.727407  P-50193,P-50040\n",
              "19  P-50293  P-50040  ...        0.675074  P-50293,P-50040\n",
              "20  P-50293  P-04563  ...        0.756557  P-50293,P-04563\n",
              "21  P-07970  P-21468  ...        0.147846  P-07970,P-21468\n",
              "\n",
              "[22 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "text": [
            "time: 40.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIwhnIue6QA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02f88b16-9e74-4b32-e597-efc6cc111442"
      },
      "source": [
        "def simi_fn(X,Y):\n",
        "  X_list = word_tokenize(X)  \n",
        "  Y_list = word_tokenize(Y) \n",
        "  sw = stopwords.words('english')  \n",
        "  l1 =[];l2 =[] \n",
        "  X_set = {w for w in X_list if not w in sw}  \n",
        "  Y_set = {w for w in Y_list if not w in sw} \n",
        "  rvector = X_set.union(Y_set)  \n",
        "  for w in rvector: \n",
        "      if w in X_set: l1.append(1) \n",
        "      else: l1.append(0) \n",
        "      if w in Y_set: l2.append(1) \n",
        "      else: l2.append(0) \n",
        "  c = 0\n",
        "  for i in range(len(rvector)): \n",
        "          c+= l1[i]*l2[i] \n",
        "  if (((sum(l1)*sum(l2))**0.5) ) == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
        "    return cosine"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 15.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ADE15_L3ulQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "f7126076-80e2-412b-d27b-5eba543fb66f"
      },
      "source": [
        "similar_df_P2 = pd.DataFrame(columns=['Id_Q1', 'Id_Q2', 'Similarity_Ques', 'Similarity_Opt', 'Similarity_Sol'])\n",
        "for k in range(df.shape[0]):\n",
        "  for l in range(k+1, df.shape[0]):\n",
        "     X =df.iloc[k,1]\n",
        "     Y =df.iloc[l,1]\n",
        "     Z =df.iloc[k,2]\n",
        "     W =df.iloc[l,2]\n",
        "     S1 = df.iloc[k,3]\n",
        "     S2 = df.iloc[l,3]\n",
        "     score1 = simi_fn(X,Y)\n",
        "     score2 = simi_fn(Z,W)\n",
        "     score3 = simi_fn(S1, S2)\n",
        "    #print(\"similarity: \", cosine) \n",
        "     if score1 > 0.7:\n",
        "      similar_df_P2 = similar_df_P2.append({'Id_Q1':df.iloc[k,0], 'Id_Q2':df.iloc[l,0], 'Similarity_Ques':score1, 'Similarity_Opt':score2, 'Similarity_Sol':score3}, ignore_index=True)\n",
        "#print(similar_df_P2)  \n",
        "similar_df_P2['combine']= similar_df_P2.Id_Q1 + ',' + similar_df_P2.Id_Q2\n",
        "similar_df_P2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id_Q1</th>\n",
              "      <th>Id_Q2</th>\n",
              "      <th>Similarity_Ques</th>\n",
              "      <th>Similarity_Opt</th>\n",
              "      <th>Similarity_Sol</th>\n",
              "      <th>combine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P-08354</td>\n",
              "      <td>P-08394</td>\n",
              "      <td>0.972973</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.804132</td>\n",
              "      <td>P-08354,P-08394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P-02491</td>\n",
              "      <td>P-02546</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.935414</td>\n",
              "      <td>P-02491,P-02546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P-01118</td>\n",
              "      <td>P-22405</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.298142</td>\n",
              "      <td>0.293470</td>\n",
              "      <td>P-01118,P-22405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P-03149</td>\n",
              "      <td>P-08402</td>\n",
              "      <td>0.889108</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.759072</td>\n",
              "      <td>P-03149,P-08402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.938971</td>\n",
              "      <td>0.774597</td>\n",
              "      <td>0.344124</td>\n",
              "      <td>P-50193,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.775672</td>\n",
              "      <td>0.258199</td>\n",
              "      <td>0.216777</td>\n",
              "      <td>P-50193,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>P-50293</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.151186</td>\n",
              "      <td>P-50293,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>P-07970</td>\n",
              "      <td>P-21468</td>\n",
              "      <td>0.949289</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>P-07970,P-21468</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Id_Q1    Id_Q2  ...  Similarity_Sol          combine\n",
              "0  P-08354  P-08394  ...        0.804132  P-08354,P-08394\n",
              "1  P-02491  P-02546  ...        0.935414  P-02491,P-02546\n",
              "2  P-01118  P-22405  ...        0.293470  P-01118,P-22405\n",
              "3  P-03149  P-08402  ...        0.759072  P-03149,P-08402\n",
              "4  P-50193  P-50293  ...        0.344124  P-50193,P-50293\n",
              "5  P-50193  P-50040  ...        0.216777  P-50193,P-50040\n",
              "6  P-50293  P-50040  ...        0.151186  P-50293,P-50040\n",
              "7  P-07970  P-21468  ...        0.000000  P-07970,P-21468\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "stream",
          "text": [
            "time: 483 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFtwXGkI8V7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "ded6b129-5d1d-4e1f-c562-64a853e67af7"
      },
      "source": [
        "mergedStuff = pd.merge(similar_df_P, similar_df_P2, on=['combine'], how='inner')\n",
        "print(mergedStuff.shape[0])\n",
        "print(similar_df_P.shape[0])\n",
        "print(similar_df_P2.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "22\n",
            "8\n",
            "time: 26.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3RuHbL-AUkN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "9314ddf1-684f-4f9d-a9d4-91438377646f"
      },
      "source": [
        "df = pd.DataFrame(columns=['Id', 'Questions', 'Options', 'Solution'])\n",
        "for filename in files_M:\n",
        "  temp=[]\n",
        "  for i in range(len(files_M_op)):\n",
        "    if files_M_op[i][-12:-7] == filename[-12:-7]:\n",
        "      gif = imageio.get_reader(files_M_op[i])\n",
        "      for frame in gif:\n",
        "          info = pytesseract.image_to_string(frame)\n",
        "          temp.append(info)\n",
        "  temp.sort()\n",
        "  temp2 = ' '.join(temp)\n",
        "  temp3=[]\n",
        "  for i in range(len(files_M_sol)):\n",
        "    if files_M_sol[i][-12:-7] == filename[-12:-7]:\n",
        "      gif = imageio.get_reader(files_M_sol[i])\n",
        "      for frame in gif:\n",
        "          info = pytesseract.image_to_string(frame)\n",
        "          temp3.append(info)\n",
        "  temp3.sort()\n",
        "  temp4 = ' '.join(temp3)\n",
        "  gif = imageio.get_reader(filename)\n",
        "  for frame in gif:\n",
        "    info = pytesseract.image_to_string(frame)\n",
        "  df = df.append({'Id':filename[-14:-7],'Questions':info, 'Options':temp2, 'Solution':temp4},ignore_index=True)\n",
        "df['Questions'].astype(str)\n",
        "print(df.head())\n",
        "similar_df_M = pd.DataFrame(columns=['Id_Q1', 'Id_Q2', 'Similarity_Ques', 'Similarity_Opt', 'Similarity_Sol'])\n",
        "for k in range(df.shape[0]):\n",
        "  for l in range(k+1, df.shape[0]):\n",
        "     X =df.iloc[k,1]\n",
        "     Y =df.iloc[l,1]\n",
        "     Z =df.iloc[k,2]\n",
        "     W =df.iloc[l,2]\n",
        "     S1 = df.iloc[k,3]\n",
        "     S2 = df.iloc[l,3]\n",
        "     score1 = simi_fn(X,Y)\n",
        "     score2 = simi_fn(Z,W)\n",
        "     score3 = simi_fn(S1, S2)\n",
        "    #print(\"similarity: \", cosine) \n",
        "     if score1 > 0.7:\n",
        "      similar_df_M = similar_df_M.append({'Id_Q1':df.iloc[k,0], 'Id_Q2':df.iloc[l,0], 'Similarity_Ques':score1, 'Similarity_Opt':score2, 'Similarity_Sol':score3}, ignore_index=True)\n",
        "print(similar_df_M) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Id  ...                                           Solution\n",
            "0  M-09045  ...  Solution:\\nf(x) =x? +bx? +ex4+d,0<b? <c\\nf(x) ...\n",
            "1  M-07594  ...  Solution:\\n\\nGiven f(x) =x + bx? +ex4+d\\n\\nF(X...\n",
            "2  M-01291  ...  Solution:\\nxy =e?\\n\\n \\n\\nLength of subnormal ...\n",
            "3  M-08446  ...  Solution:\\n\\nr=. (x) +In g(x)-2x\\n\\n£2 =F) +n)...\n",
            "4  M-07490  ...  Solution:\\nGive g(x) =x° In(x° f(x)\\n\\nDiffere...\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "     Id_Q1    Id_Q2  Similarity_Ques  Similarity_Opt  Similarity_Sol\n",
            "0  M-08446  M-08446         1.000000             1.0        1.000000\n",
            "1  M-08446  M-07670         0.714286             0.0        0.312348\n",
            "2  M-01289  M-00036         0.848875             1.0        0.380952\n",
            "3  M-08446  M-07670         0.714286             0.0        0.312348\n",
            "4  M-08363  M-07821         1.000000             1.0        0.935971\n",
            "time: 1min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZh4XTZA8pQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a5ee4da6-ac96-46e6-a143-ac9ee28e5569"
      },
      "source": [
        "df = pd.DataFrame(columns=['Id', 'Questions', 'Options', 'Solution'])\n",
        "for filename in files_B:\n",
        "  temp=[]\n",
        "  for i in range(len(files_B_op)):\n",
        "    if files_B_op[i][-12:-7] == filename[-12:-7]:\n",
        "      gif = imageio.get_reader(files_B_op[i])\n",
        "      for frame in gif:\n",
        "          info = pytesseract.image_to_string(frame)\n",
        "          temp.append(info)\n",
        "  temp.sort()\n",
        "  temp2 = ' '.join(temp)\n",
        "  temp3=[]\n",
        "  for i in range(len(files_B_sol)):\n",
        "    if files_B_sol[i][-12:-7] == filename[-12:-7]:\n",
        "      gif = imageio.get_reader(files_B_sol[i])\n",
        "      for frame in gif:\n",
        "          info = pytesseract.image_to_string(frame)\n",
        "          temp3.append(info)\n",
        "  temp3.sort()\n",
        "  temp4 = ' '.join(temp3)\n",
        "  gif = imageio.get_reader(filename)\n",
        "  for frame in gif:\n",
        "    info = pytesseract.image_to_string(frame)\n",
        "  df = df.append({'Id':filename[-14:-7],'Questions':info, 'Options':temp2, 'Solution':temp4},ignore_index=True)\n",
        "df['Questions'].astype(str)\n",
        "print(df.head())\n",
        "similar_df_B = pd.DataFrame(columns=['Id_Q1', 'Id_Q2', 'Similarity_Ques', 'Similarity_Opt', 'Similarity_Sol'])\n",
        "for k in range(df.shape[0]):\n",
        "  for l in range(k+1, df.shape[0]):\n",
        "     X =df.iloc[k,1]\n",
        "     Y =df.iloc[l,1]\n",
        "     Z =df.iloc[k,2]\n",
        "     W =df.iloc[l,2]\n",
        "     S1 = df.iloc[k,3]\n",
        "     S2 = df.iloc[l,3]\n",
        "     score1 = simi_fn(X,Y)\n",
        "     score2 = simi_fn(Z,W)\n",
        "     score3 = simi_fn(S1, S2)\n",
        "    #print(\"similarity: \", cosine) \n",
        "     if score1 > 0.7:\n",
        "      similar_df_B = similar_df_B.append({'Id_Q1':df.iloc[k,0], 'Id_Q2':df.iloc[l,0], 'Similarity_Ques':score1, 'Similarity_Opt':score2, 'Similarity_Sol':score3}, ignore_index=True)\n",
        "print(similar_df_B) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Id  ... Solution\n",
            "0  B-00476  ...         \n",
            "1  B-00486  ...         \n",
            "2  B-14749  ...         \n",
            "3  B-15922  ...         \n",
            "4  B-50131  ...         \n",
            "\n",
            "[5 rows x 4 columns]\n",
            "     Id_Q1    Id_Q2  Similarity_Ques  Similarity_Opt Similarity_Sol\n",
            "0  B-50131  B-01388         0.737865             0.0              0\n",
            "1  B-01022  B-14639         0.857143             1.0              0\n",
            "2  B-01229  B-01229         1.000000             1.0              0\n",
            "3  B-05343  B-05343         1.000000             1.0              0\n",
            "4  B-05253  B-05253         1.000000             1.0              0\n",
            "time: 1min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj4mKyY-BZyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "c532bbf2-2293-4fd2-f918-041c87eedaab"
      },
      "source": [
        "df = pd.DataFrame(columns=['Id', 'Questions', 'Options', 'Solution'])\n",
        "for filename in files_C:\n",
        "  temp=[]\n",
        "  for i in range(len(files_C_op)):\n",
        "    if files_C_op[i][-12:-7] == filename[-12:-7]:\n",
        "      gif = imageio.get_reader(files_C_op[i])\n",
        "      for frame in gif:\n",
        "          info = pytesseract.image_to_string(frame)\n",
        "          temp.append(info)\n",
        "  temp.sort()\n",
        "  temp2 = ' '.join(temp)\n",
        "  temp3=[]\n",
        "  for i in range(len(files_C_sol)):\n",
        "    if files_C_sol[i][-12:-7] == filename[-12:-7]:\n",
        "      gif = imageio.get_reader(files_C_sol[i])\n",
        "      for frame in gif:\n",
        "          info = pytesseract.image_to_string(frame)\n",
        "          temp3.append(info)\n",
        "  temp3.sort()\n",
        "  temp4 = ' '.join(temp3)\n",
        "  gif = imageio.get_reader(filename)\n",
        "  for frame in gif:\n",
        "    info = pytesseract.image_to_string(frame)\n",
        "  df = df.append({'Id':filename[-14:-7],'Questions':info, 'Options':temp2, 'Solution':temp4},ignore_index=True)\n",
        "df['Questions'].astype(str)\n",
        "print(df.head())\n",
        "similar_df_C = pd.DataFrame(columns=['Id_Q1', 'Id_Q2', 'Similarity_Ques', 'Similarity_Opt', 'Similarity_Sol'])\n",
        "for k in range(df.shape[0]):\n",
        "  for l in range(k+1, df.shape[0]):\n",
        "     X =df.iloc[k,1]\n",
        "     Y =df.iloc[l,1]\n",
        "     Z =df.iloc[k,2]\n",
        "     W =df.iloc[l,2]\n",
        "     S1 = df.iloc[k,3]\n",
        "     S2 = df.iloc[l,3]\n",
        "     score1 = simi_fn(X,Y)\n",
        "     score2 = simi_fn(Z,W)\n",
        "     score3 = simi_fn(S1, S2)\n",
        "    #print(\"similarity: \", cosine) \n",
        "     if score1 > 0.7:\n",
        "      similar_df_C = similar_df_C.append({'Id_Q1':df.iloc[k,0], 'Id_Q2':df.iloc[l,0], 'Similarity_Ques':score1, 'Similarity_Opt':score2, 'Similarity_Sol':score3}, ignore_index=True)\n",
        "print(similar_df_C) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Id  ...                                           Solution\n",
            "0  C-11109  ...  Solution:\\n\\nAt equivalent point\\n\\npH= tips +...\n",
            "1  C-06387  ...  Solution:\\n\\nAt equivalent point\\n\\npH= ; [pKw...\n",
            "2  C-06403  ...  Solution:\\nLet mole fraction of O2 is x\\n\\n40 ...\n",
            "3  C-10244  ...  Solution:\\nLet mole fraction of\\n\\nCenters wd ...\n",
            "4  C-07024  ...  Solution:\\n\\n \\n\\nd{H,0,]\\n— SEES =k [Oo]!\\ndt...\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "     Id_Q1    Id_Q2  Similarity_Ques  Similarity_Opt  Similarity_Sol\n",
            "0  C-11109  C-06387         0.764217        0.577350        0.781661\n",
            "1  C-06403  C-10244         0.867149        0.866025        0.670820\n",
            "2  C-06372  C-11122         0.769484        0.258199        0.524142\n",
            "3  C-03481  C-11106         0.823529        0.668994        0.354787\n",
            "4  C-11126  C-11127         1.000000        0.000000        0.500000\n",
            "5  C-07267  C-00225         0.925476        0.833333        0.783349\n",
            "6  C-11245  C-05143         0.870388        0.000000        0.735767\n",
            "7  C-06076  C-10281         0.923077        0.866025        0.426401\n",
            "time: 1min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv01JfZ_-BjB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad5f2ef-2eb1-4e7f-84cb-7acbe403ece4"
      },
      "source": [
        "image_path = '/content/drive/My Drive/Duplicate_Algo_280620/Results'\n",
        "count=1\n",
        "for j in range(len(similar_df_P)):\n",
        "  for i in range(len(files_P)):\n",
        "    if files_P[i][-14:-7] == similar_df_P.iloc[j,0]:\n",
        "      A = plt.imread(files_P[i])\n",
        "    if files_P[i][-14:-7] == similar_df_P.iloc[j,1]:\n",
        "      B = plt.imread(files_P[i])\n",
        "  width = A.shape[1]\n",
        "  height = A.shape[0]\n",
        "  dsize = (width, height)\n",
        "  output = cv2.resize(B, dsize)\n",
        "  C = np.hstack((A, output))\n",
        "  #cv2_imshow(C)\n",
        "  cv2.imwrite(os.path.join(image_path,('P' + str(count) + '_' +  (str(similar_df_P.iloc[j,2]))[:5]+\".jpg\")),C)\n",
        "  count+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 142 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozGBO0nrS6VX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8c2b00-f1ff-440e-e0fc-01a06219458e"
      },
      "source": [
        "image_path = '/content/drive/My Drive/Duplicate_Algo_280620/Results'\n",
        "count=1\n",
        "for j in range(len(similar_df_M)):\n",
        "  for i in range(len(files_M)):\n",
        "    if files_M[i][-14:-7] == similar_df_M.iloc[j,0]:\n",
        "      A = plt.imread(files_M[i])\n",
        "    if files_M[i][-14:-7] == similar_df_M.iloc[j,1]:\n",
        "      B = plt.imread(files_M[i])\n",
        "  width = A.shape[1]\n",
        "  height = A.shape[0]\n",
        "  dsize = (width, height)\n",
        "  output = cv2.resize(B, dsize)\n",
        "  C = np.hstack((A, output))\n",
        "  #cv2_imshow(C)\n",
        "  cv2.imwrite(os.path.join(image_path,('M' + str(count) + '_' +  (str(similar_df_M.iloc[j,2]))[:5]+\".jpg\")),C)\n",
        "  count+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 48.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE55aW3_TCsU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0582b1-9a3c-4f28-b21a-025d3e2d205f"
      },
      "source": [
        "image_path = '/content/drive/My Drive/Duplicate_Algo_280620/Results'\n",
        "count=1\n",
        "for j in range(len(similar_df_B)):\n",
        "  for i in range(len(files_B)):\n",
        "    if files_B[i][-14:-7] == similar_df_B.iloc[j,0]:\n",
        "      A = plt.imread(files_B[i])\n",
        "    if files_B[i][-14:-7] == similar_df_B.iloc[j,1]:\n",
        "      B = plt.imread(files_B[i])\n",
        "  width = A.shape[1]\n",
        "  height = A.shape[0]\n",
        "  dsize = (width, height)\n",
        "  output = cv2.resize(B, dsize)\n",
        "  C = np.hstack((A, output))\n",
        "  #cv2_imshow(C)\n",
        "  cv2.imwrite(os.path.join(image_path,('B' + str(count) + '_' +  (str(similar_df_B.iloc[j,2]))[:5]+\".jpg\")),C)\n",
        "  count+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 42.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxaH23jXTJ56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8982ee7b-9e2a-4166-c12a-09416bdad976"
      },
      "source": [
        "image_path = '/content/drive/My Drive/Duplicate_Algo_280620/Results'\n",
        "count=1\n",
        "for j in range(len(similar_df_C)):\n",
        "  for i in range(len(files_C)):\n",
        "    if files_C[i][-14:-7] == similar_df_C.iloc[j,0]:\n",
        "      A = plt.imread(files_C[i])\n",
        "    if files_C[i][-14:-7] == similar_df_C.iloc[j,1]:\n",
        "      B = plt.imread(files_C[i])\n",
        "  width = A.shape[1]\n",
        "  height = A.shape[0]\n",
        "  dsize = (width, height)\n",
        "  output = cv2.resize(B, dsize)\n",
        "  C = np.hstack((A, output))\n",
        "  #cv2_imshow(C)\n",
        "  cv2.imwrite(os.path.join(image_path,('C' + str(count) + '_' +  (str(similar_df_C.iloc[j,2]))[:5]+\".jpg\")),C)\n",
        "  count+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 60.2 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEXutxHLBGx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98947140-c198-4878-f095-b6628e5923ab"
      },
      "source": [
        "dfs = [similar_df_P,similar_df_B,similar_df_M,similar_df_C]\n",
        "result = pd.concat(dfs, ignore_index=True)\n",
        "result['Similarity_Sol'] = result['Similarity_Sol'].astype(float)\n",
        "result = result.round(2)\n",
        "(result)\n",
        "#result.to_csv('result.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id_Q1</th>\n",
              "      <th>Id_Q2</th>\n",
              "      <th>Similarity_Ques</th>\n",
              "      <th>Similarity_Opt</th>\n",
              "      <th>Similarity_Sol</th>\n",
              "      <th>combine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P-08354</td>\n",
              "      <td>P-08394</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.82</td>\n",
              "      <td>P-08354,P-08394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-04899</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.47</td>\n",
              "      <td>P-00202,P-04899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-50193</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.86</td>\n",
              "      <td>P-00202,P-50193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.90</td>\n",
              "      <td>P-00202,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P-00202</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.61</td>\n",
              "      <td>P-00202,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>P-04406</td>\n",
              "      <td>P-50273</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.37</td>\n",
              "      <td>P-04406,P-50273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>P-04406</td>\n",
              "      <td>P-01118</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.23</td>\n",
              "      <td>P-04406,P-01118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>P-02491</td>\n",
              "      <td>P-02546</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.99</td>\n",
              "      <td>P-02491,P-02546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>P-01118</td>\n",
              "      <td>P-22405</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.58</td>\n",
              "      <td>P-01118,P-22405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>P-03149</td>\n",
              "      <td>P-08402</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.95</td>\n",
              "      <td>P-03149,P-08402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>P-02551</td>\n",
              "      <td>P-02504</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.66</td>\n",
              "      <td>P-02551,P-02504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>P-02551</td>\n",
              "      <td>P-50193</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>P-02551,P-50193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>P-02551</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.76</td>\n",
              "      <td>P-02551,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>P-02551</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.45</td>\n",
              "      <td>P-02551,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>P-02504</td>\n",
              "      <td>P-50193</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.75</td>\n",
              "      <td>P-02504,P-50193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>P-02504</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.75</td>\n",
              "      <td>P-02504,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>P-02504</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.54</td>\n",
              "      <td>P-02504,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-50293</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.95</td>\n",
              "      <td>P-50193,P-50293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>P-50193</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.73</td>\n",
              "      <td>P-50193,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>P-50293</td>\n",
              "      <td>P-50040</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.68</td>\n",
              "      <td>P-50293,P-50040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>P-50293</td>\n",
              "      <td>P-04563</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.76</td>\n",
              "      <td>P-50293,P-04563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>P-07970</td>\n",
              "      <td>P-21468</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.15</td>\n",
              "      <td>P-07970,P-21468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>B-50131</td>\n",
              "      <td>B-01388</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>B-01022</td>\n",
              "      <td>B-14639</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>B-01229</td>\n",
              "      <td>B-01229</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>B-05343</td>\n",
              "      <td>B-05343</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>B-05253</td>\n",
              "      <td>B-05253</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>M-08446</td>\n",
              "      <td>M-08446</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>M-08446</td>\n",
              "      <td>M-07670</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>M-01289</td>\n",
              "      <td>M-00036</td>\n",
              "      <td>0.85</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.38</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>M-08446</td>\n",
              "      <td>M-07670</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>M-08363</td>\n",
              "      <td>M-07821</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>C-11109</td>\n",
              "      <td>C-06387</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.78</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>C-06403</td>\n",
              "      <td>C-10244</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.67</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>C-06372</td>\n",
              "      <td>C-11122</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.52</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>C-03481</td>\n",
              "      <td>C-11106</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.35</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>C-11126</td>\n",
              "      <td>C-11127</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>C-07267</td>\n",
              "      <td>C-00225</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.78</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>C-11245</td>\n",
              "      <td>C-05143</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.74</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>C-06076</td>\n",
              "      <td>C-10281</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.43</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Id_Q1    Id_Q2  ...  Similarity_Sol          combine\n",
              "0   P-08354  P-08394  ...            0.82  P-08354,P-08394\n",
              "1   P-00202  P-04899  ...            0.47  P-00202,P-04899\n",
              "2   P-00202  P-50193  ...            0.86  P-00202,P-50193\n",
              "3   P-00202  P-50293  ...            0.90  P-00202,P-50293\n",
              "4   P-00202  P-50040  ...            0.61  P-00202,P-50040\n",
              "5   P-04406  P-50273  ...            0.37  P-04406,P-50273\n",
              "6   P-04406  P-01118  ...            0.23  P-04406,P-01118\n",
              "7   P-02491  P-02546  ...            0.99  P-02491,P-02546\n",
              "8   P-01118  P-22405  ...            0.58  P-01118,P-22405\n",
              "9   P-03149  P-08402  ...            0.95  P-03149,P-08402\n",
              "10  P-02551  P-02504  ...            0.66  P-02551,P-02504\n",
              "11  P-02551  P-50193  ...            0.68  P-02551,P-50193\n",
              "12  P-02551  P-50293  ...            0.76  P-02551,P-50293\n",
              "13  P-02551  P-50040  ...            0.45  P-02551,P-50040\n",
              "14  P-02504  P-50193  ...            0.75  P-02504,P-50193\n",
              "15  P-02504  P-50293  ...            0.75  P-02504,P-50293\n",
              "16  P-02504  P-50040  ...            0.54  P-02504,P-50040\n",
              "17  P-50193  P-50293  ...            0.95  P-50193,P-50293\n",
              "18  P-50193  P-50040  ...            0.73  P-50193,P-50040\n",
              "19  P-50293  P-50040  ...            0.68  P-50293,P-50040\n",
              "20  P-50293  P-04563  ...            0.76  P-50293,P-04563\n",
              "21  P-07970  P-21468  ...            0.15  P-07970,P-21468\n",
              "22  B-50131  B-01388  ...            0.00              NaN\n",
              "23  B-01022  B-14639  ...            0.00              NaN\n",
              "24  B-01229  B-01229  ...            0.00              NaN\n",
              "25  B-05343  B-05343  ...            0.00              NaN\n",
              "26  B-05253  B-05253  ...            0.00              NaN\n",
              "27  M-08446  M-08446  ...            1.00              NaN\n",
              "28  M-08446  M-07670  ...            0.31              NaN\n",
              "29  M-01289  M-00036  ...            0.38              NaN\n",
              "30  M-08446  M-07670  ...            0.31              NaN\n",
              "31  M-08363  M-07821  ...            0.94              NaN\n",
              "32  C-11109  C-06387  ...            0.78              NaN\n",
              "33  C-06403  C-10244  ...            0.67              NaN\n",
              "34  C-06372  C-11122  ...            0.52              NaN\n",
              "35  C-03481  C-11106  ...            0.35              NaN\n",
              "36  C-11126  C-11127  ...            0.50              NaN\n",
              "37  C-07267  C-00225  ...            0.78              NaN\n",
              "38  C-11245  C-05143  ...            0.74              NaN\n",
              "39  C-06076  C-10281  ...            0.43              NaN\n",
              "\n",
              "[40 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "stream",
          "text": [
            "time: 42 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}